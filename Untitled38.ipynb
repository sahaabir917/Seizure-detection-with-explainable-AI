{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528651fe-c8bb-4241-8bd9-65768baf9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "TRAIN_NPZ = \"merged_eeg_dataset.npz\"\n",
    "TEST_NPZ  = \"test_eeg_dataset.npz\"\n",
    "\n",
    "VAL_FRAC_OF_TRAIN = 0.10\n",
    "EPOCHS = 50\n",
    "BATCH = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "L2W = 5e-4\n",
    "DROPOUT = 0.45\n",
    "LABEL_SMOOTH = 0.05\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d4ebd-64a0-48b7-a143-7306a0f51ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    X, y = d[\"X\"], d[\"y\"]\n",
    "    channels = d[\"channels\"] if \"channels\" in d.files else None\n",
    "    session_ids = d[\"session_ids\"] if \"session_ids\" in d.files else None\n",
    "    print(channels)\n",
    "    return X, y.astype(np.int32), channels, session_ids\n",
    "\n",
    "def ensure_time_feature_shape(X):\n",
    "    \"\"\"Ensure shape (N, 1280, 20).\"\"\"\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D, got {X.ndim}D\")\n",
    "    n, a, b = X.shape\n",
    "    if a == 20 and b == 1280:\n",
    "        return np.transpose(X, (0, 2, 1)).astype(np.float32)\n",
    "    elif a == 1280 and b == 20:\n",
    "        return X.astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized shape {X.shape}\")\n",
    "\n",
    "def fit_channelwise_zscore(X):\n",
    "    mean = X.mean(axis=(0, 1), keepdims=True)\n",
    "    std  = X.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "    return mean, std\n",
    "\n",
    "def apply_channelwise_zscore(X, mean, std):\n",
    "    return (X - mean) / std\n",
    "\n",
    "def per_segment_standardize(X, eps=1e-8):\n",
    "    m = X.mean(axis=1, keepdims=True)\n",
    "    s = X.std(axis=1, keepdims=True) + eps\n",
    "    return (X - m) / s\n",
    "\n",
    "def to_tf_dataset(X, y, batch_size=64, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X.astype(np.float32), y.astype(np.int32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(\n",
    "            buffer_size=min(len(X), 10000),\n",
    "            seed=SEED,\n",
    "            reshuffle_each_iteration=True\n",
    "        )\n",
    "    return ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "def build_cnn_lstm(input_shape=(1280, 20),\n",
    "                   l2w=5e-4,\n",
    "                   dropout=0.45,\n",
    "                   label_smooth=0.05,\n",
    "                   lr=3e-4):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Conv block 1\n",
    "    x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2w))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)   # seq len roughly â†’ 1280 / 2 / 2\n",
    "\n",
    "\n",
    "    x = layers.Conv1D(128, kernel_size=5, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2w))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv1D(256, kernel_size=3, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2w))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences=False)\n",
    "    )(x)\n",
    "\n",
    "    # Dense head\n",
    "    x = layers.Dense(64, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(l2w))(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smooth)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name=\"auc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3eefd-5cc6-4320-9270-0f9a0b49ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, y_train_all, train_channels, train_sessions = load_npz(TRAIN_NPZ)\n",
    "X_test,      y_test,      test_channels,  test_sessions  = load_npz(TEST_NPZ)\n",
    "\n",
    "print(\"Train raw:\", X_train_all.shape, y_train_all.shape)\n",
    "print(\"Test  raw:\", X_test.shape,      y_test.shape)\n",
    "\n",
    "X_train_all = ensure_time_feature_shape(X_train_all)\n",
    "X_test      = ensure_time_feature_shape(X_test)\n",
    "print(\"Fixed shapes -> Train:\", X_train_all.shape, \"| Test:\", X_test.shape)\n",
    "\n",
    "\n",
    "# Train and Validation Split \n",
    "\n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=VAL_FRAC_OF_TRAIN,\n",
    "    random_state=SEED\n",
    ")\n",
    "tr_idx, val_idx = next(sss.split(X_train_all, y_train_all))\n",
    "\n",
    "X_tr,  y_tr  = X_train_all[tr_idx],  y_train_all[tr_idx]\n",
    "X_val, y_val = X_train_all[val_idx], y_train_all[val_idx]\n",
    "\n",
    "print(\"Train:\", X_tr.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17310e-6b25-4920-8aad-f1c078f3cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "\n",
    "mu, sigma = fit_channelwise_zscore(X_tr)\n",
    "\n",
    "X_tr_n  = per_segment_standardize(apply_channelwise_zscore(X_tr,  mu, sigma))\n",
    "X_val_n = per_segment_standardize(apply_channelwise_zscore(X_val, mu, sigma))\n",
    "X_te_n  = per_segment_standardize(apply_channelwise_zscore(X_test, mu, sigma))\n",
    "\n",
    "# Class Weights\n",
    "\n",
    "classes = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "print(\"Class weights:\", class_weight)\n",
    "\n",
    "# tf.data Datasets\n",
    "\n",
    "train_ds = to_tf_dataset(X_tr_n,  y_tr,  batch_size=BATCH, shuffle=True)\n",
    "val_ds   = to_tf_dataset(X_val_n, y_val, batch_size=BATCH, shuffle=False)\n",
    "test_ds  = to_tf_dataset(X_te_n,  y_test, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model = build_cnn_lstm(\n",
    "    input_shape=X_tr_n.shape[1:],\n",
    "    l2w=L2W,\n",
    "    dropout=DROPOUT,\n",
    "    label_smooth=LABEL_SMOOTH,\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "es  = callbacks.EarlyStopping(\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    monitor='val_auc',\n",
    "    mode='max'\n",
    ")\n",
    "rlr = callbacks.ReduceLROnPlateau(\n",
    "    patience=4,\n",
    "    factor=0.5,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    min_lr=1e-6\n",
    ")\n",
    "ckp = callbacks.ModelCheckpoint(\n",
    "    \"best_cnn_lstm.keras\",\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es, rlr, ckp],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Threshold tuning on calidation dataset\n",
    "\n",
    "val_probs = model.predict(val_ds, verbose=0).ravel()\n",
    "y_val_all = np.concatenate([y.numpy() for _, y in val_ds], axis=0)\n",
    "\n",
    "best_thr, best_f1 = 0.5, -1\n",
    "for thr in np.linspace(0.05, 0.95, 19):\n",
    "    preds = (val_probs >= thr).astype(int)\n",
    "    f1 = f1_score(y_val_all, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_thr, best_f1 = thr, f1\n",
    "\n",
    "print(f\"\\n[VAL] Best Threshold = {best_thr:.2f} | F1 = {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_probs = model.predict(test_ds, verbose=0).ravel()\n",
    "test_preds = (test_probs >= best_thr).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7612e0f-3001-43b0-9602-ad998f2267e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc  = accuracy_score(y_test, test_preds)\n",
    "f1   = f1_score(y_test, test_preds)\n",
    "prec = precision_score(y_test, test_preds)\n",
    "rec  = recall_score(y_test, test_preds)\n",
    "auc  = roc_auc_score(y_test, test_probs)\n",
    "cm   = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "print(\"\\n=== CNN + LSTM TEST METRICS ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"ROC-AUC  : {auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, test_preds, digits=4))\n",
    "\n",
    "\n",
    "# np.savez(\"norm_stats_cnn_lstm.npz\", mu=mu, sigma=sigma)\n",
    "# print(\"\\nSaved: best_cnn_lstm.keras and norm_stats_cnn_lstm.npz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af638f-d1ee-4b0c-806d-c504ce2bf165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"SHAP for global explaination\")\n",
    "\n",
    "X_all = np.concatenate([X_tr_n, X_val_n], axis=0)\n",
    "\n",
    "print(\"Total samples used for SHAP:\", X_all.shape[0])   # (N_total, 1280, 20)\n",
    "\n",
    "\n",
    "bg_size = min(100, X_tr_n.shape[0])\n",
    "idx_bg = np.random.choice(X_tr_n.shape[0], size=bg_size, replace=False)\n",
    "background = X_tr_n[idx_bg]      \n",
    "print(\"Background shape:\", background.shape)\n",
    "\n",
    "\n",
    "explainer = shap.GradientExplainer(model, background)\n",
    "\n",
    "\n",
    "exp_size = min(500, X_all.shape[0])\n",
    "X_exp = X_all[:exp_size]\n",
    "print(\"Explain samples shape:\", X_exp.shape)\n",
    "\n",
    "print(f\"Computing SHAP values for {exp_size} segments...\")\n",
    "shap_values = explainer.shap_values(X_exp)\n",
    "\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "# print(\"Raw SHAP values shape:\", shap_values.shape) \n",
    "\n",
    "\n",
    "if shap_values.ndim == 4 and shap_values.shape[-1] == 1:\n",
    "    shap_values = shap_values[..., 0]   # (N, 1280, 20)\n",
    "\n",
    "# print(\"Fixed SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "\n",
    "channel_importance = np.mean(np.abs(shap_values), axis=(0, 1))   # shape: (20,)\n",
    "channel_importance = np.asarray(channel_importance).reshape(-1)  # ensure 1D\n",
    "\n",
    "\n",
    "print(\"\\nChannel-wise global importance (mean |SHAP| per channel):\")\n",
    "for ch_idx, imp in enumerate(channel_importance):\n",
    "    ch_name = f\"Ch{ch_idx}\"\n",
    "    if 'train_channels' in globals() and train_channels is not None and ch_idx < len(train_channels):\n",
    "        ch_name = str(train_channels[ch_idx])\n",
    "    imp_val = float(imp)   # numpy scalar -> normal float\n",
    "    print(f\"{ch_name:>6}: {imp_val:.6f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "x_pos = np.arange(len(channel_importance))\n",
    "\n",
    "labels = (\n",
    "    train_channels\n",
    "    if ('train_channels' in globals()\n",
    "        and train_channels is not None\n",
    "        and len(train_channels) == len(channel_importance))\n",
    "    else [f\"Ch{idx}\" for idx in range(len(channel_importance))]\n",
    ")\n",
    "\n",
    "plt.bar(x_pos, channel_importance)\n",
    "plt.xticks(x_pos, labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Global EEG Channel Importance (CNN + LSTM + SHAP)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd146c52-64be-41ac-a1fd-50f5c5578695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explainer \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0   \n",
    "\n",
    "\n",
    "x_sample = X_te_n[idx:idx+1]  \n",
    "\n",
    "print(f\"\\n=== Single-sample explanation for test index {idx} ===\")\n",
    "\n",
    "\n",
    "x_tensor = tf.convert_to_tensor(x_sample)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tensor)\n",
    "    y_pred = model(x_tensor)     \n",
    "\n",
    "grads = tape.gradient(y_pred, x_tensor).numpy() \n",
    "\n",
    "\n",
    "chan_importance = np.mean(np.abs(grads[0]), axis=0)  \n",
    "\n",
    "\n",
    "print(\"Per-channel importance for a single segment\")\n",
    "for ch_idx, imp in enumerate(chan_importance):\n",
    "    ch_name = f\"Ch{ch_idx}\"\n",
    "    if 'train_channels' in globals() and train_channels is not None and ch_idx < len(train_channels):\n",
    "        ch_name = str(train_channels[ch_idx])\n",
    "    print(f\"{ch_name:>6}: {imp:.6f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "x_pos = np.arange(len(chan_importance))\n",
    "\n",
    "labels = (\n",
    "    train_channels\n",
    "    if ('train_channels' in globals()\n",
    "        and train_channels is not None\n",
    "        and len(train_channels) == len(chan_importance))\n",
    "    else [f\"Ch{idx}\" for idx in range(len(chan_importance))]\n",
    ")\n",
    "\n",
    "plt.bar(x_pos, chan_importance)\n",
    "plt.xticks(x_pos, labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Mean |Gradient| over time\")\n",
    "plt.title(f\"Channel importance for test sample #{idx}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
